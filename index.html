<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Saeed Ghorbani</title> <meta name="author" content="Saeed Ghorbani"/> <meta name="description" content="Research Scientist at Amazon Games specializing in machine learning, computer vision, computer graphics, and computer animation. Expert in probabilistic motion modeling and deep learning for human motion synthesis. "/> <meta name="keywords" content="machine learning, computer vision, computer graphics, computer animation, human motion modeling, probabilistic models, deep learning, research scientist, Amazon Games, motion synthesis, pose estimation, gesture generation"/> <meta property="og:site_name" content="Saeed Ghorbani"/> <meta property="og:type" content="website"/> <meta property="og:title" content="Saeed Ghorbani | About"/> <meta property="og:url" content="https://saeed1262.github.io/"/> <meta property="og:description" content="Research Scientist at Amazon Games specializing in machine learning, computer vision, computer graphics, and computer animation. Expert in probabilistic motion modeling and deep learning for human motion synthesis. "/> <meta property="og:image" content="/assets/img/saeed.jpg"/> <meta property="og:locale" content="en"/> <meta name="twitter:card" content="summary"/> <meta name="twitter:title" content="About"/> <meta name="twitter:description" content="Research Scientist at Amazon Games specializing in machine learning, computer vision, computer graphics, and computer animation. Expert in probabilistic motion modeling and deep learning for human motion synthesis. "/> <meta name="twitter:image" content="/assets/img/saeed.jpg"/> <meta name="twitter:site" content="@SaGhorbani"/> <meta name="twitter:creator" content="@SaGhorbani"/> <script type="application/ld+json">
      {
        "author":
        {
          "@type": "Person",
          "name": "Saeed  Ghorbani"
        },
        "url": "https://saeed1262.github.io/",
        "@type": "WebSite",
        "description": "Research Scientist at Amazon Games specializing in machine learning, computer vision, computer graphics, and computer animation. Expert in probabilistic motion modeling and deep learning for human motion synthesis.
",
        "headline": "About",
        "sameAs": ["https://scholar.google.com/citations?user=JFRY_g8AAAAJ&hl", "https://github.com/saeed1262", "https://www.linkedin.com/in/saeed-ghorbani-ba4872136", "https://twitter.com/SaGhorbani"],
        "name": "Saeed  Ghorbani",
        "@context": "https://schema.org"
      }
    </script> <link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin> <link rel="preconnect" href="https://fonts.googleapis.com" crossorigin> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link rel="dns-prefetch" href="//www.google-analytics.com"> <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests"> <meta name="theme-color" content="#6c63ff"> <meta name="apple-mobile-web-app-capable" content="yes"> <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"> <meta name="msapplication-TileColor" content="#6c63ff"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="preload" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Roboto+Slab:wght@100;300;400;500;700&display=swap" as="style" onload="this.onload=null;this.rel='stylesheet'"> <noscript><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&family=Roboto+Slab:wght@100;300;400;500;700&display=swap"></noscript> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/fav.png"/> <link rel="preload" href="/assets/css/main.css" as="style" onload="this.onload=null;this.rel='stylesheet'"> <noscript><link rel="stylesheet" href="/assets/css/main.css"></noscript> <link rel="canonical" href="https://saeed1262.github.io/"> <meta name="robots" content="index, follow, max-image-preview:large, max-snippet:-1, max-video-preview:-1"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%73%61%65%65%64.%67%68%6F%72%62%61%6E%69%31%32%36%32%20%61%74%20%67%6D%61%69%6C%20%64%6F%74%20%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=JFRY_g8AAAAJ&amp;hl" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/saeed1262" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/saeed-ghorbani-ba4872136" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/SaGhorbani" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">Blog</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV</a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Submenus</a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item" href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item" href="/projects/">projects</a> </div> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Saeed Ghorbani </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"> <figure> <picture> <img class="img-fluid z-dept-1 rounded" src="/assets/img/saeed.jpg" width="auto" height="auto" alt="saeed.jpg"> </picture> </figure> <div class="address"> <p><strong>Research Scientist</strong></p> <p>Amazon Games</p> <p>Toronto, Canada</p> <p><i class="fas fa-envelope"></i> <a href="mailto:saeed.ghorbani1262@gmail.com">Contact</a></p> </div> </div> <div class="clearfix"> <p>I am a Research Scientist at <a href="https://www.amazongames.com/en-us" target="_blank" rel="noopener noreferrer">Amazon Games</a>, specializing in <strong>machine learning</strong>, <strong>computer vision</strong>, <strong>computer graphics</strong>, and <strong>computer animation</strong>. Previously, I worked as an R&amp;D Scientist at <a href="https://montreal.ubisoft.com/en/our-engagements/research-and-development/" target="_blank" rel="noopener noreferrer">Ubisoft La Forge</a>, where I contributed to cutting-edge research in motion synthesis and gesture generation.</p> <p>I earned my PhD from <a href="https://www.yorku.ca/" target="_blank" rel="noopener noreferrer">York University</a>, where I was a trainee with both the <a href="https://vista.info.yorku.ca/" target="_blank" rel="noopener noreferrer">VISTA</a> and <a href="https://cvr.yorku.ca/" target="_blank" rel="noopener noreferrer">CVR</a> programs. My research focuses on developing <strong>novel deep probabilistic models</strong> for realistic human motion modeling, with applications in character animation, pose estimation, and gesture synthesis.</p> </div> <div class="news"> <h2>News</h2> <div class="table-responsive" style="max-height: 10vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jul 19, 2024</th> <td> <p>Our <a href="https://arxiv.org/abs/2404.14634" target="_blank" rel="noopener noreferrer">paper</a> on human pose estimation with cross-view and temporal cues has been accepted to ECCV 2024.</p> </td> </tr> <tr> <th scope="row">May 15, 2023</th> <td> <a class="news-title" href="/news/announcement_17/">ZeroEGGS Featured in Ubisoft La Forge Blog</a> </td> </tr> <tr> <th scope="row">Mar 13, 2023</th> <td> <p><a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS" target="_blank" rel="noopener noreferrer">Zero-shot Example-based Gesture Generation from Speech</a> is published at Computer Graphics Forum. <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/cgf.14734" target="_blank" rel="noopener noreferrer">Link to the paper</a></p> </td> </tr> <tr> <th scope="row">Jan 15, 2023</th> <td> <a class="news-title" href="/news/announcement_18/">Joined Amazon Games as Research Scientist</a> </td> </tr> <tr> <th scope="row">Nov 28, 2022</th> <td> <p><a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS" target="_blank" rel="noopener noreferrer">ZEGGS</a> is featured in <a href="https://www.youtube.com/watch?v=Dt0cA2phKfU&amp;t=17s" target="_blank" rel="noopener noreferrer">Two Minute Papers</a> channel.</p> </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected publications</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ECCV</abbr></div> <div id="davoodnia2024upose3d" class="col-sm-8"> <div class="title">UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues</div> <div class="author"> Vandad Davoodnia, <em>Saeed Ghorbani</em>, <a href="https://sites.google.com/site/marcandrecarbonneau/" target="_blank" rel="noopener noreferrer">Marc-André Carbonneau</a>, Alexandre Messier, and <a href="http://alietemad.com/" target="_blank" rel="noopener noreferrer">Ali Etemad</a> </div> <div class="periodical"> <em>European Conference on Computer Vision (ECCV)</em> 2024</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/abs/2404.14634" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://vdavoodnia.github.io/projects/upose3d/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Abstract. We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields performance rivalling methods that rely on 3D annotated data while being the state-of-the-art among methods relying only on 2D supervision.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">davoodnia2024upose3d</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{UPose3D: Uncertainty-Aware 3D Human Pose Estimation with Cross-View and Temporal Cues}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davoodnia, Vandad and Ghorbani, Saeed and Carbonneau, Marc-Andr{\'e} and Messier, Alexandre and Etemad, Ali}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{European Conference on Computer Vision (ECCV)}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ECCVW</abbr></div> <div id="davoodnia2024skelformer" class="col-sm-8"> <div class="title">SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers</div> <div class="author"> Vandad Davoodnia, <em>Saeed Ghorbani</em>, Alexandre Messier, and <a href="http://alietemad.com/" target="_blank" rel="noopener noreferrer">Ali Etemad</a> </div> <div class="periodical"> <em>ECCVW Workshop on Video Games</em> 2024</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2404.12625" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://vdavoodnia.github.io/projects/skelformer/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Abstract. We introduce UPose3D, a novel approach for multi-view 3D human pose estimation, addressing challenges in accuracy and scalability. Our method advances existing pose estimation frameworks by improving robustness and flexibility without requiring direct 3D annotations. At the core of our method, a pose compiler module refines predictions from a 2D keypoints estimator that operates on a single image by leveraging temporal and cross-view information. Our novel cross-view fusion strategy is scalable to any number of cameras, while our synthetic data generation strategy ensures generalization across diverse actors, scenes, and viewpoints. Finally, UPose3D leverages the prediction uncertainty of both the 2D keypoint estimator and the pose compiler module. This provides robustness to outliers and noisy data, resulting in state-of-the-art performance in out-of-distribution settings. In addition, for in-distribution settings, UPose3D yields performance rivalling methods that rely on 3D annotated data while being the state-of-the-art among methods relying only on 2D supervision.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">davoodnia2024skelformer</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{SkelFormer: Markerless 3D Pose and Shape Estimation using Skeletal Transformers}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Davoodnia, Vandad and Ghorbani, Saeed and Messier, Alexandre and Etemad, Ali}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{ECCVW Workshop on Video Games}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CGF</abbr></div> <div id="ghorbani2023zeroeggs" class="col-sm-8"> <div class="title">ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech</div> <div class="author"> <em>Saeed Ghorbani</em>, Ylva Ferstl, <a href="https://theorangeduck.com/" target="_blank" rel="noopener noreferrer">Daniel Holden</a>, <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank" rel="noopener noreferrer">Nikolaus F. Troje</a>, and <a href="https://sites.google.com/site/marcandrecarbonneau/" target="_blank" rel="noopener noreferrer">Marc-André Carbonneau</a> </div> <div class="periodical"> <em>Computer Graphics Forum</em> 2023</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14734" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://www.ubisoft.com/en-us/studio/laforge/news/5ADkkY0BMG9vNSDuUMtkeg/zeroeggs-zeroshot-examplebased-gesture-generation-from-speech" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Blog</a> <a href="https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://saeed1262.github.io/projects/zeggs/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>Abstract We present ZeroEGGS, a neural network framework for speech-driven gesture generation with zero-shot style control by example. This means style can be controlled via only a short example motion clip, even for motion styles unseen during training. Our model uses a Variational framework to learn a style embedding, making it easy to modify style through latent space manipulation or blending and scaling of style embeddings. The probabilistic nature of our framework further enables the generation of a variety of outputs given the input, addressing the stochastic nature of gesture motion. In a series of experiments, we first demonstrate the flexibility and generalizability of our model to new speakers and styles. In a user study, we then show that our model outperforms previous state-of-the-art techniques in naturalness of motion, appropriateness for speech, and style portrayal. Finally, we release a high-quality dataset of full-body gesture motion including fingers, with speech, spanning across 19 different styles. Our code and data are publicly available at https://github.com/ubisoft/ubisoft-laforge-ZeroEGGS.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghorbani2023zeroeggs</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghorbani, Saeed and Ferstl, Ylva and Holden, Daniel and Troje, Nikolaus F. and Carbonneau, Marc-Andr{\'e}}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{ZeroEGGS: Zero-shot Example-based Gesture Generation from Speech}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Graphics Forum}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">keywords</span> <span class="p">=</span> <span class="s">{animation, gestures, character control, motion capture}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{https://doi.org/10.1111/cgf.14734}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://onlinelibrary.wiley.com/doi/abs/10.1111/cgf.14734}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CGF</abbr></div> <div id="ghorbani2020b" class="col-sm-8"> <div class="title">Probabilistic Character Motion Synthesis using a Hierarchical Deep Latent Variable Model</div> <div class="author"> <em>Saeed Ghorbani</em>, <a href="http://www.cse.yorku.ca/~calden/" target="_blank" rel="noopener noreferrer">Calden Wloka</a>, <a href="http://alietemad.com/" target="_blank" rel="noopener noreferrer">Ali Etemad</a>, <a href="https://mbrubake.github.io/" target="_blank" rel="noopener noreferrer">Marcus A. Brubaker</a>, and <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank" rel="noopener noreferrer">Nikolaus F. Troje</a> </div> <div class="periodical"> <em>Computer Graphics Forum (Symposium on Computer Aanimation)</em> 2020</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/2010.09950.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://saeed1262.github.io/projects/motionModel/" class="btn btn-sm z-depth-0" role="button">Website</a> </div> <div class="abstract hidden"> <p>We present a probabilistic framework to generate character animations based on weak control signals, such that the synthesized motions are realistic while retaining the stochastic nature of human movement. The proposed architecture, which is designed as a hierarchical recurrent model, maps each sub-sequence of motions into a stochastic latent code using a variational autoencoder extended over the temporal domain. We also propose an objective function which respects the impact of each joint on the pose and compares the joint angles based on angular distance. We use two novel quantitative protocols and human qualitative assessment to demonstrate the ability of our model to generate convincing and diverse periodic and non-periodic motion sequences without the need for strong control signals.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghorbani2020b</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Computer Graphics Forum (Symposium on Computer Aanimation)}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{Probabilistic Character Motion Synthesis using a Hierarchical Deep Latent Variable Model}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghorbani, Saeed and Wloka, Calden and Etemad, Ali and Brubaker, Marcus A. and Troje, Nikolaus F.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2020}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{The Eurographics Association and John Wiley &amp; Sons Ltd.}</span><span class="p">,</span>
  <span class="na">issn</span> <span class="p">=</span> <span class="s">{1467-8659}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.1111/cgf.14116}</span><span class="p">,</span>
  <span class="na">talk</span> <span class="p">=</span> <span class="s">{https://www.youtube.com/watch?v=r9F74LcGC0A}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">PLOS ONE</abbr></div> <div id="ghorbani2020movi" class="col-sm-8"> <div class="title">MoVi: A large multi-purpose human motion and video dataset</div> <div class="author"> <em>Saeed Ghorbani</em>, Kimia Mahdaviani, Anne Thaler, <a href="http://koerding.com/" target="_blank" rel="noopener noreferrer">Konrad Kording</a>, Douglas James Cook, <a href="http://www.compneurosci.com/people.html" target="_blank" rel="noopener noreferrer">Gunnar Blohm</a>, and <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank" rel="noopener noreferrer">Nikolaus F. Troje</a> </div> <div class="periodical"> <em>Plos one</em> 2021</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://journals.plos.org/plosone/article/file?id=10.1371/journal.pone.0253157&amp;type=printable" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> <a href="https://github.com/saeed1262/MoVi-Toolbox" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Code</a> <a href="https://www.biomotionlab.ca/movi/" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">Website</a> </div> <div class="abstract hidden"> <p>Human movements are both an area of intense study and the basis of many applications such as character animation. For many applications, it is crucial to identify movements from videos or analyze datasets of movements. Here we introduce a new human Motion and Video dataset MoVi, which we make available publicly. It contains 60 female and 30 male actors performing a collection of 20 predefined everyday actions and sports movements, and one self-chosen movement. In five capture rounds, the same actors and movements were recorded using different hardware systems, including an optical motion capture system, video cameras, and inertial measurement units (IMU). For some of the capture rounds, the actors were recorded when wearing natural clothing, for the other rounds they wore minimal clothing. In total, our dataset contains 9 hours of motion capture data, 17 hours of video data from 4 different points of view (including one hand-held camera), and 6.6 hours of IMU data. In this paper, we describe how the dataset was collected and post-processed; We present state-of-the-art estimates of skeletal motions and full-body shape deformations associated with skeletal motion. We discuss examples for potential studies this dataset could enable.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">ghorbani2020movi</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MoVi: A large multi-purpose human motion and video dataset}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghorbani, Saeed and Mahdaviani, Kimia and Thaler, Anne and Kording, Konrad and Cook, Douglas James and Blohm, Gunnar and Troje, Nikolaus F.}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2021}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{Plos one}</span><span class="p">,</span>
  <span class="na">primaryclass</span> <span class="p">=</span> <span class="s">{cs.CV}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">CGI</abbr></div> <div id="ghorbani2019auto" class="col-sm-8"> <div class="title">Auto-labelling of markers in optical motion capture by permutation learning</div> <div class="author"> <em>Saeed Ghorbani</em>, <a href="http://alietemad.com/" target="_blank" rel="noopener noreferrer">Ali Etemad</a>, and <a href="https://www.biomotionlab.ca/niko-troje/" target="_blank" rel="noopener noreferrer">Nikolaus F. Troje</a> </div> <div class="periodical"> <em>In Computer Graphics International</em> 2019</div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://arxiv.org/pdf/1907.13580.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank" rel="noopener noreferrer">PDF</a> </div> <div class="abstract hidden"> <p>Optical marker-based motion capture is a vital tool in applications such as motion and behavioural analysis, animation, and biomechanics. Labelling, that is, assigning optical markers to the pre-defined positions on the body is a time consuming and labour intensive postprocessing part of current motion capture pipelines. The problem can be considered as a ranking process in which markers shuffled by an unknown permutation matrix are sorted to recover the correct order. In this paper, we present a framework for automatic marker labelling which first estimates a permutation matrix for each individual frame using a differentiable permutation learning model and then utilizes temporal consistency to identify and correct remaining labelling errors. Experiments conducted on the test data show the effectiveness of our framework.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghorbani2019auto</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Auto-labelling of markers in optical motion capture by permutation learning}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghorbani, Saeed and Etemad, Ali and Troje, Nikolaus F.}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Computer Graphics International}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{167--178}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2019}</span><span class="p">,</span>
  <span class="na">organization</span> <span class="p">=</span> <span class="s">{Springer}</span><span class="p">,</span>
  <span class="na">award</span> <span class="p">=</span> <span class="s">{Best Paper Award}</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%73%61%65%65%64.%67%68%6F%72%62%61%6E%69%31%32%36%32%20%61%74%20%67%6D%61%69%6C%20%64%6F%74%20%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=JFRY_g8AAAAJ&amp;hl" title="Google Scholar" target="_blank" rel="noopener noreferrer"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/saeed1262" title="GitHub" target="_blank" rel="noopener noreferrer"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/saeed-ghorbani-ba4872136" title="LinkedIn" target="_blank" rel="noopener noreferrer"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/SaGhorbani" title="Twitter" target="_blank" rel="noopener noreferrer"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> The best way to reach me is through email or LinkedIn. </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2025 Saeed Ghorbani. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="noopener noreferrer">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" target="_blank" rel="noopener noreferrer">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="noopener noreferrer">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="noopener noreferrer">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script async src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="/assets/js/common.js"></script> <script defer src="/assets/js/custom_effects.js"></script> <script type="text/javascript">window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]],displayMath:[["$$","$$"],["\\[","\\]"]],processEscapes:!0,processEnvironments:!0,tags:"ams"},options:{ignoreHtmlClass:"tex2jax_ignore",processHtmlClass:"tex2jax_process"}};</script> <script type="text/javascript" id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> </body> </html>